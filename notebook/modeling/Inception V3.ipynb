{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networks tried earlier didn't gave the best results. So, let's try running the Inception V3 network inspired by https://github.com/stratospark/food-101-keras/blob/master/Food%20Classification%20with%20Deep%20Learning%20in%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import stat\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used image augmentation in modeling.py file we can use multiprocessing.pool to accelerate image augmentation during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code loads all the data in the memory in one go, instead we are interested in loading the data in batches as we are working on a much smaller RAM size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import math\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "n_classes = 101\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "x = base_model.output\n",
    "x = AveragePooling2D(pool_size=(8, 8))(x)\n",
    "x = Dropout(.4)(x)\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(n_classes, init='glorot_uniform', W_regularizer=l2(.0005), activation='softmax')(x)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "opt = SGD(lr=.01, momentum=.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='../../model/model4.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('../../logs/model4.log')\n",
    "\n",
    "def schedule(epoch):\n",
    "    if epoch < 15:\n",
    "        return .01\n",
    "    elif epoch < 28:\n",
    "        return .002\n",
    "    else:\n",
    "        return .0004\n",
    "lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "# mixing the old code into GoogleNet\n",
    "# original sixe of the batch size was 64 but due to the limitation of the GPU memory the batch size is decreased. \n",
    "batch_size = 32\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False, # randomly flip images\n",
    "    zoom_range=[.8, 1],\n",
    "    channel_shift_range=30,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '../../data/raw/food-101/smallersample/train/',  # this is the target directory\n",
    "    target_size=(299, 299),  # all images will be resized to 299x299\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode='categorical')  \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    '../../data/raw/food-101/smallersample/test/',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# model.fit_generator(\n",
    "#     train_generator,\n",
    "#     validation_data=validation_generator,\n",
    "#     validation_steps=25250 // batch_size,\n",
    "#     steps_per_epoch=75750 // batch_size,\n",
    "#     epochs=32,\n",
    "#     callbacks=[lr_scheduler, csv_logger, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code works fine but is hard to train on the GPUs I have i.e 1070M due to heating issues and 980Ti due to the small memory size. Let's try to run the code on Goggle colab or any other platform like AWS Sagemaker.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model Evaluation</h4>\n",
    "\n",
    "After training the code on Google colab. We have weights of the trained model. Let's load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../../model/model4.08-0.67.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make prediction for an image, we need to know what is the id of the images. Going by the assumption that model trained on colab used the same ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Images, display\n",
    "\n",
    "display(Images(filename=\"../../data/raw/food-101/images/fried_rice/260614.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the above image of fried rice and make prediction. As can be seen from cell above, the predicted id should be 44. But before that we need to convert the image data such that it could be understood by th model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "\n",
    "# to check if the image is of shape 299 x 299. if not then resize to this shape. \n",
    "min_side=299\n",
    "img_arr = img.imread(\"../../data/raw/food-101/images/fried_rice/260614.jpg\")\n",
    "img_arr_rs = img_arr\n",
    "\n",
    "try:\n",
    "    w, h, _ = img_arr.shape\n",
    "    if w < min_side:\n",
    "        wpercent = (min_side/float(w))\n",
    "        hsize = int((float(h)*float(wpercent)))\n",
    "        #print('new dims:', min_side, hsize)\n",
    "        img_arr_rs = imresize(img_arr, (min_side, hsize))\n",
    "        resize_count += 1\n",
    "    elif h < min_side:\n",
    "        hpercent = (min_side/float(h))\n",
    "        wsize = int((float(w)*float(hpercent)))\n",
    "        #print('new dims:', wsize, min_side)\n",
    "        img_arr_rs = imresize(img_arr, (wsize, min_side))\n",
    "        resize_count += 1\n",
    "except:\n",
    "    print('Skipping bad image')\n",
    "    \n",
    "# cropping the image to be 299 x 299\n",
    "imageData = center_crop(img_arr_rs, (299, 299))\n",
    "# changing the shape of the imageData to fit the prediction\n",
    "imageData = imageData[np.newaxis,:,:,:]\n",
    "\n",
    "y_pred = model.predict(imageData)\n",
    "preds = np.argmax(y_pred, axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yipppeeee! we got index 44 which is the index of fried rice. So it worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to evaluate the test set using multiple crops. This is expected to raise the accuracy by 5% compared to single crop evaluation. It is common to use the following crops: Upper Left, Upper Right, Lower Left, Lower Right, Center. We also take the same crops on the image flipped left to right, creating a total of 10 crops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(x, center_crop_size, **kwargs):\n",
    "    centerw, centerh = x.shape[0]//2, x.shape[1]//2\n",
    "    halfw, halfh = center_crop_size[0]//2, center_crop_size[1]//2\n",
    "    return x[centerw-halfw:centerw+halfw+1,centerh-halfh:centerh+halfh+1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_10_crop(img, top_n=5, plot=False, preprocess=True, debug=False):\n",
    "    flipped_X = np.fliplr(img)\n",
    "    crops = [\n",
    "        img[:299,:299, :], # Upper Left\n",
    "        img[:299, img.shape[1]-299:, :], # Upper Right\n",
    "        img[img.shape[0]-299:, :299, :], # Lower Left\n",
    "        img[img.shape[0]-299:, img.shape[1]-299:, :], # Lower Right\n",
    "        center_crop(img, (299, 299)),\n",
    "        \n",
    "        flipped_X[:299,:299, :],\n",
    "        flipped_X[:299, flipped_X.shape[1]-299:, :],\n",
    "        flipped_X[flipped_X.shape[0]-299:, :299, :],\n",
    "        flipped_X[flipped_X.shape[0]-299:, flipped_X.shape[1]-299:, :],\n",
    "        center_crop(flipped_X, (299, 299))\n",
    "    ]\n",
    "    if preprocess:\n",
    "        crops = [preprocess_input(x.astype('float32')) for x in crops]\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "        ax[0][0].imshow(crops[0])\n",
    "        ax[0][1].imshow(crops[1])\n",
    "        ax[0][2].imshow(crops[2])\n",
    "        ax[0][3].imshow(crops[3])\n",
    "        ax[0][4].imshow(crops[4])\n",
    "        ax[1][0].imshow(crops[5])\n",
    "        ax[1][1].imshow(crops[6])\n",
    "        ax[1][2].imshow(crops[7])\n",
    "        ax[1][3].imshow(crops[8])\n",
    "        ax[1][4].imshow(crops[9])\n",
    "    \n",
    "    y_pred = model.predict(np.array(crops))\n",
    "    preds = np.argmax(y_pred, axis=1)\n",
    "    top_n_preds= np.argpartition(y_pred, -top_n)[:,-top_n:]\n",
    "    if debug:\n",
    "        print('Top-1 Predicted:', preds)\n",
    "        print('Top-5 Predicted:', top_n_preds)\n",
    "    return preds, top_n_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it worked on this pad thai image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Images, display\n",
    "\n",
    "display(Images(filename=\"../../data/abc.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a prediction now and get top-1 and top-5 accuracy for 10 cropped images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as matimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "allowedAspectratio = 1.3\n",
    "\n",
    "img = Image.open('../../data/abc.jpg')\n",
    "\n",
    "mywidth = 400\n",
    "wpercent = (mywidth/float(img.size[0]))\n",
    "hsize = int((float(img.size[1])*float(wpercent)))\n",
    "img = img.resize((mywidth,hsize), PIL.Image.ANTIALIAS)\n",
    "img.save('../../data/resized.jpg')\n",
    "\n",
    "im = Image.open('../../data/resized.jpg')\n",
    "width, height = im.size   # Get dimensions\n",
    "if(height > width):\n",
    "    if((height/width) > allowedAspectratio):\n",
    "        extraHeight = height - (width * 1.3)\n",
    "        top = extraHeight/2\n",
    "        bottom = height - extraHeight/2\n",
    "        im = im.crop((0, top, width, bottom))\n",
    "        im.save('../../data/resized.jpg')\n",
    "else:\n",
    "    if((width/height) > allowedAspectratio):\n",
    "        extraWidth = width - (height * 1.3)\n",
    "        left = extraWidth/2\n",
    "        right = width - extraWidth/2\n",
    "        im = im.crop((left, 0, right, height))\n",
    "        im.save('../../data/resized.jpg')\n",
    "        \n",
    "img_arr = matimg.imread(\"../../data/resized.jpg\")\n",
    "prediction, topNPrediction = predict_10_crop(img_arr, top_n=5, plot=True, preprocess=False, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We got the predictions. Let's see what is the most common prediction in all 10 crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(prediction)\n",
    "mostCommonPrediction = np.argmax(counts)\n",
    "print(mostCommonPrediction)\n",
    "\n",
    "labelDictonary = train_generator.class_indices\n",
    "print(list(labelDictonary.keys())[list(labelDictonary.values()).index(mostCommonPrediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

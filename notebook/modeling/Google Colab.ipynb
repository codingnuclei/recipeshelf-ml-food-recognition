{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes inspiration from this colab notebook: https://colab.research.google.com/drive/1Z5-LqlJxNslHcCzM9jChsNhGi9Y7HSC4. Due to the limitation of the resources we are going to train the code on Google colab. \n",
    "\n",
    "<b>Google colab</b> helps us to write and execute python in the browser. We don't have to configure anything as almost everything come pre-configured and most importantly we get free access to GPUs. These GPUs however are shared so, its not necessary that we always get the access but almost all the time apart from once or twice I got the GPUs on colab.\n",
    "\n",
    "We get a linux box running for a session which typically last for 12 hours if there is a job running in background. In the free tier of the instance we usually get 12.7 GB of system RAM, around 37 GB of disk space and most importantly NVIDIA K80 graphic card. Which is pretty awesome to run the job with higher batch sizes. I tried running the job on my 1070M and 980Ti Chips which are very powerful chips but these chips suffer from a lower RAM size. \n",
    "\n",
    "Finally, one more very important thing to remember is to dock the google drive to your colab project. Which is very simple to do. This will help you to save the learning milestones to google drive otherwise the data go away once session ends. Or you can manually download the hdf5 files, which is definitely doesn't sound nice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the GPU is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>device:GPU:0</b> Indicates that the GPU was assigned. If you don't see this, it means that there are no GPU available right now. Try this some other time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download the data and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to download data and extract\n",
    "import os\n",
    "def get_data_extract():\n",
    "    if \"food-101\" in os.listdir():\n",
    "        print(\"Dataset already exists\")\n",
    "    else:\n",
    "        print(\"Downloading the data...\")\n",
    "        !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "        print(\"Dataset downloaded!\")\n",
    "        print(\"Extracting data..\")\n",
    "        !tar xzvf food-101.tar.gz\n",
    "        print(\"Extraction done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing all the directories in the dataset\n",
    "import os\n",
    "os.listdir('food-101/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from inceptionV3 notebook that an easy way is to read the data from Train and Test directories. So let's read the train and test data from the json files that come along with the dataset and create new folder separating train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to split dataset into train and test folders\n",
    "\n",
    "from shutil import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "def prepare_data(filepath, src, dest):\n",
    "    classes_images = defaultdict(list)\n",
    "    with open(filepath, 'r') as txt:\n",
    "        paths = [read.strip() for read in txt.readlines()]\n",
    "        for p in paths:\n",
    "            food = p.split('/')\n",
    "            classes_images[food[0]].append(food[1] + '.jpg')\n",
    "            \n",
    "    for food in classes_images.keys():\n",
    "        print(\"\\nCopying images into \",food)\n",
    "        if not os.path.exists(os.path.join(dest,food)):\n",
    "            os.makedirs(os.path.join(dest,food))\n",
    "        for i in classes_images[food]:\n",
    "            copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
    "\n",
    "    print(\"Copying Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n",
    "print(\"Creating train data...\")\n",
    "prepare_data('food-101/meta/train.txt', 'food-101/images', 'food-101/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\n",
    "print(\"Creating test data...\")\n",
    "prepare_data('food-101/meta/test.txt', 'food-101/images', 'food-101/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of samples to varify the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many files are in the test folder\n",
    "print(\"Total number of samples in train folder\")\n",
    "!find food-101/train -type d -or -type f -printf '.' | wc -c\n",
    "\n",
    "print(\"Total number of samples in test folder\")\n",
    "!find food-101/test -type d -or -type f -printf '.' | wc -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import stat\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining code remains the same as the InceptionV3 expect that we store the checkpoints in Google drive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import math\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "n_classes = 101\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "x = base_model.output\n",
    "x = AveragePooling2D(pool_size=(8, 8))(x)\n",
    "x = Dropout(.4)(x)\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(n_classes, init='glorot_uniform', W_regularizer=l2(.0005), activation='softmax')(x)\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "opt = SGD(lr=.01, momentum=.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='drive/My Drive/Deeplearning/model/model4.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('drive/My Drive/Deeplearning/log/model4.log')\n",
    "\n",
    "def schedule(epoch):\n",
    "    if epoch < 15:\n",
    "        return .01\n",
    "    elif epoch < 28:\n",
    "        return .002\n",
    "    else:\n",
    "        return .0004\n",
    "lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "# mixing the old code into GoogleNet\n",
    "# original sixe of the batch size was 64 but due to the limitation of the GPU memory the batch size is decreased. \n",
    "batch_size = 64\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False, # randomly flip images\n",
    "    zoom_range=[.8, 1],\n",
    "    channel_shift_range=30,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'food-101/train/',  # this is the target directory\n",
    "    target_size=(299, 299),  # all images will be resized to 299x299\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode='categorical')  \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'food-101/test/',\n",
    "    target_size=(299, 299),\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.load_weights('drive/My Drive/Deeplearning/model/model4.08-0.71.hdf5')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=25250 // batch_size,\n",
    "    steps_per_epoch=75750 // batch_size,\n",
    "    epochs=32,\n",
    "    callbacks=[lr_scheduler, csv_logger, checkpointer])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

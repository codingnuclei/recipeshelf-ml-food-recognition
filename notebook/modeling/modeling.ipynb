{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the different machine learning models on which the training data of the <i>food-101</i> dataset will be trained on. Algorithms with different complexities will be used here. Its always a good practice to start with simplest model and later trying complex ones. But before we get our hands dirty with modeling, one more step lies between EDA and modeling which is Feature Engineering. In a usual scenario, feature engineering should get its separate notebook but because the dataset is already clean, images are already arranged in proper folders, all food items have 1000 images(except for one data object as seen in EDA), and data is well split into training and test set so, there is not much to do in feature engineering. Also, if we have to make some changes in the dataset it might be based on the model we choose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the first model.\n",
    "# Support Vector Machine \n",
    "\n",
    "Support vector machine is discriminative classifier formally defined by a separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>SVM</i> is one of the simples models that we can you for classification. Images of different size could impact the learning of <i>SVM</i>. However, this is just an assumption. To see if this assumption holds we can train SVM on 2 datasets and evaluate the performance. To achieve this let's create a copy dataset where all the images are stored as square and of size <b>300x300</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the rectangular images to square can be achieved through two ways. Either by shrinking the dimensions or cutting them out. Resizing the dimension will keep all the information but will move the image away from real world example. For example, let's see how the smallest image in the dataset will look like if we resize it to be square. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Original Image</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "image = Image.open('../../data/raw/food-101/images/macarons/3247436.jpg')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resized Image</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Taking square root of the length * breath\n",
    "sqrWidth = np.ceil(np.sqrt(image.size[0] * image.size[1])).astype(int) \n",
    "im_resize = image.resize((sqrWidth, sqrWidth))\n",
    "plt.imshow(im_resize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite bad but still holds the information about the food. Let's see what happens when we cut the extra dimensions to make the image square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new square white image with dimenion equal to smaller side of original image\n",
    "# then paste the original image over the white image\n",
    "def make_square(image, max_size=600, fill_color=(0, 0, 0)):\n",
    "    x, y = image.size\n",
    "    size = min(max_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(image, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im\n",
    "\n",
    "new_image = make_square(image)\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more like a real image, infact this image removes noise from the original image. However, we lose information while cropping the image. \n",
    "\n",
    "For the later method of cropping an image we can do a little variation and create a new kind of square image. Instead of using the smaller side of the image, we can use the longer one and fill the extra space with black or white color to generate a square image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new square white image with dimenion equal to smaller side of original image\n",
    "# then paste the original image over the white image\n",
    "def make_big_square(image, min_size=50, fill_color=(0, 0, 0)):\n",
    "    x, y = image.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(image, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im\n",
    "\n",
    "new_image = make_big_square(image)\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This keeps all the information and convert the image to a square but also adds a lot more information. We don't know yest, whether this helps with learning or not. We can create new dataset of images in this format as well to compare performance of algorithm on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the resized image, shrining the longer dimension up to a certain length makes sense. If ratio of dimension is very high then the resizing can be far from realism. Let's see how many of the images in the dataset have ratio of more than 2:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "path = '../../data/raw/food-101/images'\n",
    "\n",
    "imageCount = 0\n",
    "fileNameList = []\n",
    "\n",
    "for r, d, f in tqdm(os.walk(path)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # dividing the longer side of image with the smaller one\n",
    "        ratio = (max(image.size[0],image.size[1]) / min(image.size[0],image.size[1])) \n",
    "        if(ratio >= 2):\n",
    "            fileNameList.append(fileName)\n",
    "            imageCount += 1\n",
    "\n",
    "print(\"Number of images with ratio more than 2:1 are-\" + str(imageCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are 47 images which have a ratio of more than 2:1, which is nothing compared to the total of 100999 images. Let's display 3 images from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Images, display\n",
    "display(Images(filename=fileNameList[0]))\n",
    "display(Images(filename=fileNameList[21]))\n",
    "display(Images(filename=fileNameList[45]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 47 images contains a lot of false images as well. Let's take a look at false images. But because the images are very few in number we don't need to delete anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename=fileNameList[1]))\n",
    "display(Images(filename=fileNameList[17]))\n",
    "display(Images(filename=fileNameList[28]))\n",
    "display(Images(filename=fileNameList[29]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, this is good that only 47 images have dimension ratio of more than 2:1 but we don't know how many images are rectangle. To do the performance check of how different algorithms behave with different image sizes and scaling, we need to have a good quantity of images with rectangle shape. Let's count the number of images which are rectangle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/raw/food-101/images'\n",
    "\n",
    "rectangleImageCount = 0\n",
    "\n",
    "for r, d, f in tqdm(os.walk(path)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        if(image.size[0] != image.size[1]):\n",
    "            rectangleImageCount += 1\n",
    "\n",
    "print(\"Number of rectangle images are: \" + str(rectangleImageCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 38793 images which are rectangle in the dataset, which is 38.4%. This number is high enough to see the change in learning performance based on different reshaping techniques. Let's start with creating first datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageShrink\n",
    "First dataset contains all square images achieved by shrinking the longer dimension to match the shorter one. We made a copy of data set and will now replace each rectangular image with a square in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/raw/food-101/imagesShrink'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(path)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # Finding the shorter dimension\n",
    "        shorterDimension = min(image.size[0],image.size[1])\n",
    "        im_resize = image.resize((shorterDimension, shorterDimension))\n",
    "        # Replacing the original images with resized one.\n",
    "        im_resize.save(fileName, 'JPEG' )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if that worked. Displaying a random rectangular image from both directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesShrink/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. Moving onto creating another dataset with longer length cropped to fit the square size. To do this, we make a copy of the images dataset by the name of imagesCrop and run the below function.\n",
    "\n",
    "### ImageCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCrop = '../../data/raw/food-101/imagesCrop/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathCrop)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # cropping the image using the make_square function used earlier\n",
    "        new_image = make_square(image)\n",
    "        # Replacing the original images with resized one.\n",
    "        new_image.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this operation was completed successfully or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesCrop/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked. Also, as we can see that the subject has been cropped out while transforming the image. This could lead to some problem if the food object is not in the center of the image. If we see a big drop of performance for cropped images, we try to find a solution with which the food object could be translated to the center before being cropped. But, for now let's create the third dataset where rectangular images are transformed to square by extending the shorter dimension to fit with the longer one. To do this, we make a copy of the images dataset by the name of imagesExtend and run the below function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImagesExtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathExtend = '../../data/raw/food-101/imagesExtend/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # cropping the image using the make_square function used earlier\n",
    "        new_image = make_big_square(image)\n",
    "        # Replacing the original images with resized one.\n",
    "        new_image.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this operation was completed successfully or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesExtend/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start with the learning part and compare the performance of SVM on these four datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this will not be that simple, each pixel is treated as a feature. So starting with the simplest configuration. We use the cropped square images and convert them to same size i.e. 100x100 pixels. Over to that, color images contain 3 extra dimensions each for red, green and blue. We can avoid that too now by converting all images to the black and white. Created a new directory with name imagesCrop100x100. Using the below function to convert all the images in that folder to size 100x100 and black&white in color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathExtend = '../../data/raw/food-101/imagesCrop100x100/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        # converting image to greyscale\n",
    "        image = Image.open(fileName).convert(\"RGB\")\n",
    "        image = image.convert('L')\n",
    "        # resizing image to 100x100\n",
    "        im_resize = image.resize((100, 100))\n",
    "        # Replacing the original images with resized one.\n",
    "        im_resize.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the image now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesCrop100x100/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks right. As computer only understands numbers let's convert the image to an array of pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "\n",
    "pathExtend = '../../data/raw/food-101/imagesCrop100x100/'\n",
    "\n",
    "count = 0\n",
    "imagesData = []\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        # reading the pixel values into a matrix\n",
    "        image2DArray = imread(fileName)\n",
    "        # converting the matrix to a 1D array\n",
    "        flattenedImage = image2DArray.flatten()\n",
    "        flattenedImage = np.array(flattenedImage)\n",
    "        # Appending the file name to the array \n",
    "        flattenedImage = np.append(flattenedImage, int(file[:-4]))\n",
    "        # Appending the class label to the array\n",
    "        flattenedImage = np.append(flattenedImage, count)\n",
    "        imagesData.append(flattenedImage)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, a lot of things happened above. Let's take a look at the values. Analyzing randomly selected 7890th image in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imagesData[7890])\n",
    "print(\"Total number of data points are: \" + str(len(imagesData[7890])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the idea about the data. There are 10002 values for the image. Which makes sense as 10000 of those are each pixel value of 100x100 Gray-scale image. Second last value is the name of the image. It can be used to identify the image. Last value in the array represents the label of the image and will work as our class on the basis of which we will do the classification. Going from <b>1</b> for <i>apple pie</i> to <b>101</b> for <i>waffles</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going by that logic, the 7890th should be an image of <i>bibimbap</i> i.e. 597420.jpg. Let's display both the images, i.e original and then gray-scale 100x100 image. Also, we can regenerate the image back from the pixel values to do the comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/bibimbap/597420.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesCrop100x100/bibimbap/597420.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this looks good. Let's regenerate the image from pixel values to see if it matches the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePixels = imagesData[7890][:-2]\n",
    "\n",
    "# Convert the pixels into an array using numpy\n",
    "imagePixels = np.array(imagePixels, dtype=np.uint8)\n",
    "\n",
    "# reshaping to a 2D array\n",
    "imagePixels = np.reshape(imagePixels, (-1, 100))\n",
    "\n",
    "# Use PIL to create an image from the new array of pixels\n",
    "new_image = Image.fromarray(imagePixels)\n",
    "# displaying image in grayscale\n",
    "plt.imshow(new_image, cmap = plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo!! The regenerated image is same as the original image. We can now move to next step i.e. splitting the data into training and test set. Luckily, the data came already split into train and test set. If we see into the <i>meta</i> directory we will see that there are 4 files, 2 json and 2 text. The json and text files contains the copy of each other in different format. Looking in to the json file, one is <i>test</i> and other is <i>train</i>.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data is present with us in pixel form in variable <b>imagesData</b> we can create 2 variables out of it called <b>trainDataList</b> and <b>testDataList</b>. We can also store these lists into files so that we can quickly load it next time. Also we are going to use the HDF5 file to store the data. When it comes to lot of data, HDF5 is very fast in reading and writing of data, compared to using the text file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "# changing the output to maxmimum size \n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Reading the train json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\train.json', 'r') as f:\n",
    "    trainDataDictionary = json.load(f)\n",
    "    \n",
    "# Writing the data to a file for reuse \n",
    "trainingDataFile = open(r'..\\..\\data\\processed\\trainingimageData.txt','w')\n",
    "\n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "trainDataList = []\n",
    "for key in tqdm(trainDataDictionary.keys()):\n",
    "    for val in trainDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                trainDataList.append(imageDataArray)\n",
    "                trainingDataFile.write(str(imageDataArray))\n",
    "                trainingDataFile.write('\\n')              \n",
    "trainingDataFile.close()\n",
    "\n",
    "# doing the same for the test data \n",
    "# Reading the test json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\test.json', 'r') as f:\n",
    "    testDataDictionary = json.load(f)\n",
    "    \n",
    "# Writing the data to a file for reuse \n",
    "testDataFile = open(r'..\\..\\data\\processed\\testimageData.txt','w')\n",
    "    \n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "testDataList = []\n",
    "for key in tqdm(testDataDictionary.keys()):\n",
    "    for val in testDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                testDataList.append(imageDataArray)\n",
    "                testDataFile.write(str(imageDataArray))\n",
    "                testDataFile.write('\\n')              \n",
    "testDataFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we are going to use the HDF5 file to store the data. When it comes to lot of data, HDF5 is very fast in reading and writing of data, compared to using the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# Reading the train json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\train.json', 'r') as f:\n",
    "    trainDataDictionary = json.load(f)\n",
    "\n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "trainShape = (75750, 10002)\n",
    "\n",
    "# Open the hdf5 file in write mode\n",
    "hdf5File = h5py.File(hdf5Path, mode='w')\n",
    "hdf5File.create_dataset(\"train_images\", trainShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Storing the data to the HDF5 file   \n",
    "for key in tqdm(trainDataDictionary.keys()):\n",
    "    for val in trainDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"train_images\"][count, ...] = imageDataArray     \n",
    "        count += 1\n",
    "\n",
    "# doing the same for the test data \n",
    "# Reading the test json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\test.json', 'r') as f:\n",
    "    testDataDictionary = json.load(f)\n",
    "    \n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "testShape = (25250, 10002)\n",
    "\n",
    "hdf5File.create_dataset(\"test_images\", testShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "for key in tqdm(testDataDictionary.keys()):\n",
    "    for val in testDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"test_images\"][count, ...] = imageDataArray  \n",
    "        count += 1\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the size of <i>testimageData</i> file is 1.85 GB and and <i>trainingimageData</i> file is 5.56 GB. These are big files. The <i>dataset.hdf5</i> that contains both the train and test data is 963 MB. This is big difference. Apart from the file size the read and write time from the .hdf5 file is insanely faster then text or .csv files when it comes to lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read back the values from the file into some variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# open the hdf5 file\n",
    "hdf5File = h5py.File(hdf5Path, \"r\")\n",
    "\n",
    "trainData = hdf5File[\"train_images\"][:]\n",
    "testData = hdf5File[\"test_images\"][:]\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was crazy fast!!! Lets' separate the label and remove image name from the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = trainData[:,-1:]\n",
    "trainDataCopy = np.copy(trainData)\n",
    "trainDataCopy = trainDataCopy[:,:-2] \n",
    "\n",
    "testLabels = testData[:,-1:]\n",
    "testDataCopy = np.copy(testData)\n",
    "testDataCopy = testDataCopy[:,:-2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Sci-kit learn library to use the SVM model to fit the data. But before that we need to change the data so that it gets easy compatibility with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to a dataframe\n",
    "trainDataCopy = pd.DataFrame(trainDataCopy)\n",
    "\n",
    "# Flatenning the labels to be 1D array\n",
    "trainLabels = trainLabels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ready to go into classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = svm.SVC(verbose=True)\n",
    "clf = GridSearchCV(svc, param_grid, verbose=True)\n",
    "clf.fit(trainDataCopy, trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a big task for SVM. When can reduce the problem size to make it easier for algorithm to learn and see the difference in performance. Instead of making it a 101 classification problem when can instead do a binary classification first. From the same dataset we can just take the data of <span style=\"color:green\"><b>Cup Cakes</b></span> and <span style=\"color:green\"><b>Donuts</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this we can read the train and test data again, but this time we can only read the values that are given for <i>donut</i> and <i>cup_cakes</i>. Let's retrieve this data from the json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the train and test json file and converting the data to a List\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\train.json', 'r') as file:\n",
    "    binaryTrainFoodJson = json.load(file)\n",
    "    \n",
    "keys = [\"donuts\", \"cup_cakes\"]\n",
    "\n",
    "binaryTrainList = [binaryTrainFoodJson.get(key) for key in keys]\n",
    "\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\test.json', 'r') as file:\n",
    "    binaryTestFoodJson = json.load(file)\n",
    "    \n",
    "keys = [\"donuts\", \"cup_cakes\"]\n",
    "\n",
    "binaryTestList = [binaryTestFoodJson.get(key) for key in keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! So, we have the train and test image data in <i>binaryTrainList</i> and <i>binaryTestList</i>. Let's store that in HDF5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "trainShape = (1500, 10002)\n",
    "\n",
    "# Open the hdf5 file in write mode\n",
    "hdf5File = h5py.File(hdf5Path, mode='w')\n",
    "hdf5File.create_dataset(\"subset_train_images\", trainShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Storing the data to the HDF5 file   \n",
    "for i in tqdm(range(2)):\n",
    "    for item in binaryTrainList[i]:\n",
    "        imageName = item.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"subset_train_images\"][count, ...] = imageDataArray     \n",
    "        count += 1\n",
    "\n",
    "# doing the same for the test data \n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "testShape = (500, 10002)\n",
    "\n",
    "hdf5File.create_dataset(\"subset_test_images\", testShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "for i in tqdm(range(2)):\n",
    "    for item in binaryTestList[i]:\n",
    "        imageName = item.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"subset_test_images\"][count, ...] = imageDataArray  \n",
    "        count += 1\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data being stored let's rerun the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# open the hdf5 file\n",
    "hdf5File = h5py.File(hdf5Path, \"r\")\n",
    "\n",
    "trainData = hdf5File[\"subset_train_images\"][:]\n",
    "testData = hdf5File[\"subset_test_images\"][:]\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating label and removing the image name from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = trainData[:,-1:]\n",
    "trainDataCopy = np.copy(trainData)\n",
    "trainDataCopy = trainDataCopy[:,:-2] \n",
    "\n",
    "testLabels = testData[:,-1:]\n",
    "testDataCopy = np.copy(testData)\n",
    "testDataCopy = testDataCopy[:,:-2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to a dataframe\n",
    "trainDataCopy = pd.DataFrame(trainDataCopy)\n",
    "testDataCopy = pd.DataFrame(testDataCopy)\n",
    "\n",
    "# Flatenning the labels to be 1D array\n",
    "trainLabels = trainLabels.flatten()\n",
    "testLabels = testLabels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = clf.predict(testDataCopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training done, lets see the performance of the training. Starting with the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got 53.4% accuracy. This is a very bad value. However, accuracy is not the best performance indicator when it comes to classification. Let's check the precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=30))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With precision being 53.23% and Recall being 56%, these are some bad values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the algorithm on a very basic configuration. Let's run the algorithm again with a different kernel and some other hyper-parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001, 0.0005, 0.005], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = svm.SVC(verbose=True)\n",
    "clf = GridSearchCV(svc, param_grid, verbose=True)\n",
    "\n",
    "%time clf.fit(trainDataCopy, trainLabels)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results we got were for {'C': 1, 'kernel': 'linear'}. Which was also the default algorithm that ran first time. Let's see how we can improve it. We can try to extract some meaningful features. We can use PCA principal component analysis to extract 200 fundamental components of the image to feed into out support vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "clf = svm.SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, clf)\n",
    "\n",
    "#Train the model using the training sets\n",
    "model.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = model.predict(testDataCopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "\n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=30))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Accuracy: 61.6%, Precision: 62.5% and Recall 58%, we see a jump in the all performance measures by using a different kernel and using 150 principle component. Lets' try a combination of hyper-parameters again on this new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(trainDataCopy, trainLabels)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best configuration turns out to be with c = 5 and gamma being 0.005. Let's see the performance with this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "testPrediction = model.predict(testDataCopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "\n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=30))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Accuracy: 61%, Precision: 61.9, Recall: 5%8 Its nearly same. In fact, it is fractionally low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to remember\n",
    "* images where we will lose the information if we crop to make it rectangular\n",
    "* using an algorithm to find the important part of food image which can be used to check the performance as well specially for cropped images.\n",
    "* Checking performance with or without data augmentation\n",
    "* after making it square, check performance with or without making all images of same size\n",
    "* use 5 different classifier atleast to know the performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with reading the datasets created for performance analysis in inception V3.ipynb notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5File = h5py.File(r'..\\..\\data\\processed\\predictions.hdf5', 'r')\n",
    "list(hdf5File.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! let's read the topPred2D dataset that contains the Top-1, 10-Crop image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction2D = hdf5File['topPred2D']\n",
    "Prediction2D[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "right_counter = 0\n",
    "for i in tqdm(range(len(Prediction2D))):\n",
    "    if Prediction2D[i][0] == Prediction2D[i][11]:\n",
    "        right_counter += 1\n",
    "        \n",
    "print('Top-1 Accuracy, 10-Crop: {0:.2f}%'.format(right_counter / len(Prediction2D) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting 78.72% accuracy in first trial. Now let's see the Top-5 accuracy, 10 crops. We are assuming that both topPred2D top5Pred3d contains prediction in same order for source image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction3DTop5 = hdf5File['top5Pred3D']\n",
    "Prediction3DTop5[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "topPred2D = np.zeros(5)\n",
    "top_5_counter = 0\n",
    "\n",
    "for i in tqdm(range(len(Prediction3DTop5))):\n",
    "    for j in range(5):\n",
    "        counts = np.bincount(list(np.transpose(Prediction3DTop5[i])[j]))\n",
    "        mostCommonPrediction = np.argmax(counts)\n",
    "        topPred2D[j] =  mostCommonPrediction\n",
    "        \n",
    "    if (Prediction2D[i][0] in topPred2D):\n",
    "        top_5_counter += 1\n",
    "        \n",
    "print('Top-5 Accuracy, 10-Crop: {0:.2f}%'.format(top_5_counter / len(Prediction2D) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Top-5 accuracy we get 87.28%. Which is a big jump. For calculating the value we used the most common value among same index of top-5 prediction for each crop. We can use another technique where we use all the values in top-5 and find the top 5 most common values of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "topPred2D = np.zeros(5)\n",
    "top_5_counter = 0\n",
    "\n",
    "for i in tqdm(range(len(Prediction3DTop5))):\n",
    "    for j in range(5):\n",
    "        mostCommonFlattenedList = Counter(list(Prediction3DTop5[i].flatten())).most_common(5)\n",
    "        topPred2D[j] =  mostCommonFlattenedList[j][0]\n",
    "      \n",
    "    if (Prediction2D[i][0] in topPred2D):\n",
    "        top_5_counter += 1\n",
    "        \n",
    "print('Top-5 Accuracy, 10-Crop: {0:.2f}%'.format(top_5_counter / len(Prediction2D) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!! Got a jump of more than 6% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n",
    "\n",
    "More than anything, at this point I am interested in ranking classes based on their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyList = [] # list to contain individual accuracies\n",
    "\n",
    "for item in tqdm(range(101)):\n",
    "    subGroup = Prediction2D[Prediction2D[:,0]==item,-1] # subGrouping data for each item\n",
    "    count = 0\n",
    "    for predIndex in range(len(subGroup)):\n",
    "        if(subGroup[predIndex] == item):\n",
    "            count += 1\n",
    "            \n",
    "    accuracy = count/len(subGroup)\n",
    "    accuracyList.append((item, accuracy))\n",
    "    \n",
    "accuracyList[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! so we have individual accuracies. let's see which index has the worst accuracy of all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in accuracyList if min(accuracyList)[1] in item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that Apple Pie and Hummus had the worst accuracy, Checking the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in accuracyList if max(accuracyList)[1] in item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chicken quesadilla, Croque madame, and Waffles had the best accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the different machine learning models on which the training data of the <i>food-101</i> dataset will be trained on. Algorithms with different complexities will be used here. Its always a good practice to start with simplest model and later trying complex ones. But before we get our hands dirty with modeling, one more step lies between EDA and modeling which is Feature Engineering. In a usual scenario, feature engineering should get its separate notebook but because the dataset is already clean, images are already arranged in proper folders, all food items have 1000 images(except for one data object as seen in EDA), and data is well split into training and test set so, there is not much to do in feature engineering. Also, if we have to make some changes in the dataset it might be based on the model we choose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the first model.\n",
    "# Support Vector Machine \n",
    "\n",
    "Support vector machine is discriminative classifier formally defined by a separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>SVM</i> is one of the simples models that we can you for classification. Images of different size could impact the learning of <i>SVM</i>. However, this is just an assumption. To see if this assumption holds we can train SVM on 2 datasets and evaluate the performance. To achieve this let's create a copy dataset where all the images are stored as square and of size <b>300x300</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the rectangular images to square can be achieved through two ways. Either by shrinking the dimensions or cutting them out. Resizing the dimension will keep all the information but will move the image away from real world example. For example, let's see how the smallest image in the dataset will look like if we resize it to be square. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Original Image</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "image = Image.open('../../data/raw/food-101/images/macarons/3247436.jpg')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resized Image</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Taking square root of the length * breath\n",
    "sqrWidth = np.ceil(np.sqrt(image.size[0] * image.size[1])).astype(int) \n",
    "im_resize = image.resize((sqrWidth, sqrWidth))\n",
    "plt.imshow(im_resize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite bad but still holds the information about the food. Let's see what happens when we cut the extra dimensions to make the image square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new square white image with dimenion equal to smaller side of original image\n",
    "# then paste the original image over the white image\n",
    "def make_square(image, max_size=600, fill_color=(0, 0, 0)):\n",
    "    x, y = image.size\n",
    "    size = min(max_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(image, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im\n",
    "\n",
    "new_image = make_square(image)\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more like a real image, infact this image removes noise from the original image. However, we lose information while cropping the image. \n",
    "\n",
    "For the later method of cropping an image we can do a little variation and create a new kind of square image. Instead of using the smaller side of the image, we can use the longer one and fill the extra space with black or white color to generate a square image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new square white image with dimenion equal to smaller side of original image\n",
    "# then paste the original image over the white image\n",
    "def make_big_square(image, min_size=50, fill_color=(0, 0, 0)):\n",
    "    x, y = image.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(image, (int((size - x) / 2), int((size - y) / 2)))\n",
    "    return new_im\n",
    "\n",
    "new_image = make_big_square(image)\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This keeps all the information and convert the image to a square but also adds a lot more information. We don't know yest, whether this helps with learning or not. We can create new dataset of images in this format as well to compare performance of algorithm on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the resized image, shrining the longer dimension up to a certain length makes sense. If ratio of dimension is very high then the resizing can be far from realism. Let's see how many of the images in the dataset have ratio of more than 2:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "path = '../../data/raw/food-101/images'\n",
    "\n",
    "imageCount = 0\n",
    "fileNameList = []\n",
    "\n",
    "for r, d, f in tqdm(os.walk(path)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # dividing the longer side of image with the smaller one\n",
    "        ratio = (max(image.size[0],image.size[1]) / min(image.size[0],image.size[1])) \n",
    "        if(ratio >= 2):\n",
    "            fileNameList.append(fileName)\n",
    "            imageCount += 1\n",
    "\n",
    "print(\"Number of images with ratio more than 2:1 are-\" + str(imageCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are 47 images which have a ratio of more than 2:1, which is nothing compared to the total of 100999 images. Let's display 3 images from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Images, display\n",
    "display(Images(filename=fileNameList[0]))\n",
    "display(Images(filename=fileNameList[21]))\n",
    "display(Images(filename=fileNameList[45]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 47 images contains a lot of false images as well. Let's take a look at false images. But because the images are very few in number we don't need to delete anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename=fileNameList[1]))\n",
    "display(Images(filename=fileNameList[17]))\n",
    "display(Images(filename=fileNameList[28]))\n",
    "display(Images(filename=fileNameList[29]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, this is good that only 47 images have dimension ratio of more than 2:1 but we don't know how many images are rectangle. To do the performance check of how different algorithms behave with different image sizes and scaling, we need to have a good quantity of images with rectangle shape. Let's count the number of images which are rectangle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/raw/food-101/images'\n",
    "\n",
    "rectangleImageCount = 0\n",
    "\n",
    "for r, d, f in tqdm(os.walk(path)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        if(image.size[0] != image.size[1]):\n",
    "            rectangleImageCount += 1\n",
    "\n",
    "print(\"Number of rectangle images are: \" + str(rectangleImageCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 38793 images which are rectangle in the dataset, which is 38.4%. This number is high enough to see the change in learning performance based on different reshaping techniques. Let's start with creating first datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageShrink\n",
    "First dataset contains all square images achieved by shrinking the longer dimension to match the shorter one. We made a copy of data set and will now replace each rectangular image with a square in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/raw/food-101/imagesShrink'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(path)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # Finding the shorter dimension\n",
    "        shorterDimension = min(image.size[0],image.size[1])\n",
    "        im_resize = image.resize((shorterDimension, shorterDimension))\n",
    "        # Replacing the original images with resized one.\n",
    "        im_resize.save(fileName, 'JPEG' )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if that worked. Displaying a random rectangular image from both directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesShrink/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. Moving onto creating another dataset with longer length cropped to fit the square size. To do this, we make a copy of the images dataset by the name of imagesCrop and run the below function.\n",
    "\n",
    "### ImageCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathCrop = '../../data/raw/food-101/imagesCrop/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathCrop)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # cropping the image using the make_square function used earlier\n",
    "        new_image = make_square(image)\n",
    "        # Replacing the original images with resized one.\n",
    "        new_image.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this operation was completed successfully or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesCrop/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked. Also, as we can see that the subject has been cropped out while transforming the image. This could lead to some problem if the food object is not in the center of the image. If we see a big drop of performance for cropped images, we try to find a solution with which the food object could be translated to the center before being cropped. But, for now let's create the third dataset where rectangular images are transformed to square by extending the shorter dimension to fit with the longer one. To do this, we make a copy of the images dataset by the name of imagesExtend and run the below function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImagesExtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathExtend = '../../data/raw/food-101/imagesExtend/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        image = Image.open(fileName)\n",
    "        # cropping the image using the make_square function used earlier\n",
    "        new_image = make_big_square(image)\n",
    "        # Replacing the original images with resized one.\n",
    "        new_image.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this operation was completed successfully or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesExtend/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start with the learning part and compare the performance of SVM on these four datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this will not be that simple, each pixel is treated as a feature. So starting with the simplest configuration. We use the cropped square images and convert them to same size i.e. 100x100 pixels. Over to that, color images contain 3 extra dimensions each for red, green and blue. We can avoid that too now by converting all images to the black and white. Created a new directory with name imagesCrop100x100. Using the below function to convert all the images in that folder to size 100x100 and black&white in color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pathExtend = '../../data/raw/food-101/imagesCrop100x100Color/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        # converting image to greyscale\n",
    "        image = Image.open(fileName).convert(\"RGB\")\n",
    "        # resizing image to 100x100\n",
    "        im_resize = image.resize((100, 100))\n",
    "        # Replacing the original images with resized one.\n",
    "        im_resize.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the black and white images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathExtend = '../../data/raw/food-101/imagesCrop100x100/'\n",
    "\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        # converting image to greyscale\n",
    "        image = Image.open(fileName).convert(\"RGB\")\n",
    "        image = image.convert('L')\n",
    "        # resizing image to 100x100\n",
    "        im_resize = image.resize((100, 100))\n",
    "        # Replacing the original images with resized one.\n",
    "        im_resize.save(fileName, 'JPEG' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the image now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/apple_pie/693210.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesCrop100x100/apple_pie/693210.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks right. As computer only understands numbers let's convert the image to an array of pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "\n",
    "pathExtend = '../../data/raw/food-101/imagesCrop100x100/'\n",
    "\n",
    "count = 0\n",
    "imagesData = []\n",
    "for r, d, f in tqdm(os.walk(pathExtend)):\n",
    "    for file in f:\n",
    "        fileName = os.path.join(r, file)\n",
    "        # reading the pixel values into a matrix\n",
    "        image2DArray = imread(fileName)\n",
    "        # converting the matrix to a 1D array\n",
    "        flattenedImage = image2DArray.flatten()\n",
    "        flattenedImage = np.array(flattenedImage)\n",
    "        # Appending the file name to the array \n",
    "        flattenedImage = np.append(flattenedImage, int(file[:-4]))\n",
    "        # Appending the class label to the array\n",
    "        flattenedImage = np.append(flattenedImage, count)\n",
    "        imagesData.append(flattenedImage)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, a lot of things happened above. Let's take a look at the values. Analyzing randomly selected 7890th image in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imagesData[7890])\n",
    "print(\"Total number of data points are: \" + str(len(imagesData[7890])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the idea about the data. There are 10002 values for the image. Which makes sense as 10000 of those are each pixel value of 100x100 Gray-scale image. Second last value is the name of the image. It can be used to identify the image. Last value in the array represents the label of the image and will work as our class on the basis of which we will do the classification. Going from <b>1</b> for <i>apple pie</i> to <b>101</b> for <i>waffles</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going by that logic, the 7890th should be an image of <i>bibimbap</i> i.e. 597420.jpg. Let's display both the images, i.e original and then gray-scale 100x100 image. Also, we can regenerate the image back from the pixel values to do the comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Images(filename='../../data/raw/food-101/images/bibimbap/597420.jpg'))\n",
    "display(Images(filename='../../data/raw/food-101/imagesCrop100x100/bibimbap/597420.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this looks good. Let's regenerate the image from pixel values to see if it matches the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePixels = imagesData[7890][:-2]\n",
    "\n",
    "# Convert the pixels into an array using numpy\n",
    "imagePixels = np.array(imagePixels, dtype=np.uint8)\n",
    "\n",
    "# reshaping to a 2D array\n",
    "imagePixels = np.reshape(imagePixels, (-1, 100))\n",
    "\n",
    "# Use PIL to create an image from the new array of pixels\n",
    "new_image = Image.fromarray(imagePixels)\n",
    "# displaying image in grayscale\n",
    "plt.imshow(new_image, cmap = plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo!! The regenerated image is same as the original image. We can now move to next step i.e. splitting the data into training and test set. Luckily, the data came already split into train and test set. If we see into the <i>meta</i> directory we will see that there are 4 files, 2 json and 2 text. The json and text files contains the copy of each other in different format. Looking in to the json file, one is <i>test</i> and other is <i>train</i>.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image data is present with us in pixel form in variable <b>imagesData</b> we can create 2 variables out of it called <b>trainDataList</b> and <b>testDataList</b>. We can also store these lists into files so that we can quickly load it next time. Also we are going to use the HDF5 file to store the data. When it comes to lot of data, HDF5 is very fast in reading and writing of data, compared to using the text file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "# changing the output to maxmimum size \n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Reading the train json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\train.json', 'r') as f:\n",
    "    trainDataDictionary = json.load(f)\n",
    "    \n",
    "# Writing the data to a file for reuse \n",
    "trainingDataFile = open(r'..\\..\\data\\processed\\trainingimageData.txt','w')\n",
    "\n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "trainDataList = []\n",
    "for key in tqdm(trainDataDictionary.keys()):\n",
    "    for val in trainDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                trainDataList.append(imageDataArray)\n",
    "                trainingDataFile.write(str(imageDataArray))\n",
    "                trainingDataFile.write('\\n')              \n",
    "trainingDataFile.close()\n",
    "\n",
    "# doing the same for the test data \n",
    "# Reading the test json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\test.json', 'r') as f:\n",
    "    testDataDictionary = json.load(f)\n",
    "    \n",
    "# Writing the data to a file for reuse \n",
    "testDataFile = open(r'..\\..\\data\\processed\\testimageData.txt','w')\n",
    "    \n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "testDataList = []\n",
    "for key in tqdm(testDataDictionary.keys()):\n",
    "    for val in testDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                testDataList.append(imageDataArray)\n",
    "                testDataFile.write(str(imageDataArray))\n",
    "                testDataFile.write('\\n')              \n",
    "testDataFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we are going to use the HDF5 file to store the data. When it comes to lot of data, HDF5 is very fast in reading and writing of data, compared to using the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# Reading the train json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\train.json', 'r') as f:\n",
    "    trainDataDictionary = json.load(f)\n",
    "\n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "trainShape = (75750, 10002)\n",
    "\n",
    "# Open the hdf5 file in write mode\n",
    "hdf5File = h5py.File(hdf5Path, mode='w')\n",
    "hdf5File.create_dataset(\"train_images\", trainShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Storing the data to the HDF5 file   \n",
    "for key in tqdm(trainDataDictionary.keys()):\n",
    "    for val in trainDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"train_images\"][count, ...] = imageDataArray     \n",
    "        count += 1\n",
    "\n",
    "# doing the same for the test data \n",
    "# Reading the test json file and converting the data to a dictionary\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\test.json', 'r') as f:\n",
    "    testDataDictionary = json.load(f)\n",
    "    \n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "testShape = (25250, 10002)\n",
    "\n",
    "hdf5File.create_dataset(\"test_images\", testShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "for key in tqdm(testDataDictionary.keys()):\n",
    "    for val in testDataDictionary[key]:\n",
    "        imageName = val.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"test_images\"][count, ...] = imageDataArray  \n",
    "        count += 1\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, the size of <i>testimageData</i> file is 1.85 GB and and <i>trainingimageData</i> file is 5.56 GB. These are big files. The <i>dataset.hdf5</i> that contains both the train and test data is 963 MB. This is big difference. Apart from the file size the read and write time from the .hdf5 file is insanely faster then text or .csv files when it comes to lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read back the values from the file into some variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# open the hdf5 file\n",
    "hdf5File = h5py.File(hdf5Path, \"r\")\n",
    "\n",
    "trainData = hdf5File[\"train_images\"][:]\n",
    "testData = hdf5File[\"test_images\"][:]\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was crazy fast!!! Lets' separate the label and remove image name from the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = trainData[:,-1:]\n",
    "trainDataCopy = np.copy(trainData)\n",
    "trainDataCopy = trainDataCopy[:,:-2] \n",
    "\n",
    "testLabels = testData[:,-1:]\n",
    "testDataCopy = np.copy(testData)\n",
    "testDataCopy = testDataCopy[:,:-2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Sci-kit learn library to use the SVM model to fit the data. But before that we need to change the data so that it gets easy compatibility with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to a dataframe\n",
    "trainDataCopy = pd.DataFrame(trainDataCopy)\n",
    "\n",
    "# Flatenning the labels to be 1D array\n",
    "trainLabels = trainLabels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ready to go into classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = svm.SVC(verbose=True)\n",
    "clf = GridSearchCV(svc, param_grid, verbose=True)\n",
    "clf.fit(trainDataCopy, trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a big task for SVM. When can reduce the problem size to make it easier for algorithm to learn and see the difference in performance. Instead of making it a 101 classification problem when can instead do a binary classification first. From the same dataset we can just take the data of <span style=\"color:green\"><b>Cup Cakes</b></span> and <span style=\"color:green\"><b>Donuts</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this we can read the train and test data again, but this time we can only read the values that are given for <i>donut</i> and <i>cup_cakes</i>. Let's retrieve this data from the json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the train and test json file and converting the data to a List\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\train.json', 'r') as file:\n",
    "    binaryTrainFoodJson = json.load(file)\n",
    "    \n",
    "keys = [\"donuts\", \"cup_cakes\"]\n",
    "\n",
    "binaryTrainList = [binaryTrainFoodJson.get(key) for key in keys]\n",
    "\n",
    "with open(r'..\\..\\data\\raw\\food-101\\meta\\test.json', 'r') as file:\n",
    "    binaryTestFoodJson = json.load(file)\n",
    "    \n",
    "keys = [\"donuts\", \"cup_cakes\"]\n",
    "\n",
    "binaryTestList = [binaryTestFoodJson.get(key) for key in keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! So, we have the train and test image data in <i>binaryTrainList</i> and <i>binaryTestList</i>. Let's store that in HDF5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "trainShape = (1500, 10002)\n",
    "\n",
    "# Open the hdf5 file in write mode\n",
    "hdf5File = h5py.File(hdf5Path, mode='w')\n",
    "hdf5File.create_dataset(\"subset_train_images\", trainShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# Storing the data to the HDF5 file   \n",
    "for i in tqdm(range(2)):\n",
    "    for item in binaryTrainList[i]:\n",
    "        imageName = item.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"subset_train_images\"][count, ...] = imageDataArray     \n",
    "        count += 1\n",
    "\n",
    "# doing the same for the test data \n",
    "# Fixing a shape for the array in which image data will be stored\n",
    "testShape = (500, 10002)\n",
    "\n",
    "hdf5File.create_dataset(\"subset_test_images\", testShape, np.uint32)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# creating a new list out of images Data that contains data of only those data that are in training set     \n",
    "for i in tqdm(range(2)):\n",
    "    for item in binaryTestList[i]:\n",
    "        imageName = item.split('/')[1]\n",
    "        for imageDataArray in imagesData:\n",
    "            if(int(imageName) == imageDataArray[-2]):\n",
    "                hdf5File[\"subset_test_images\"][count, ...] = imageDataArray  \n",
    "        count += 1\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data being stored let's rerun the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address to store the HDF5 file \n",
    "hdf5Path = r'..\\..\\data\\processed\\dataset.hdf5'\n",
    "\n",
    "# open the hdf5 file\n",
    "hdf5File = h5py.File(hdf5Path, \"r\")\n",
    "\n",
    "trainData = hdf5File[\"subset_train_images\"][:]\n",
    "testData = hdf5File[\"subset_test_images\"][:]\n",
    "\n",
    "hdf5File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating label and removing the image name from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabels = trainData[:,-1:]\n",
    "trainDataCopy = np.copy(trainData)\n",
    "trainDataCopy = trainDataCopy[:,:-2] \n",
    "\n",
    "testLabels = testData[:,-1:]\n",
    "testDataCopy = np.copy(testData)\n",
    "testDataCopy = testDataCopy[:,:-2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Shuffle the data for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "trainDataCopy, trainLabels = shuffle(trainDataCopy, trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to a dataframe\n",
    "trainDataCopy = pd.DataFrame(trainDataCopy)\n",
    "testDataCopy = pd.DataFrame(testDataCopy)\n",
    "\n",
    "# Flatenning the labels to be 1D array\n",
    "trainLabels = trainLabels.flatten()\n",
    "testLabels = testLabels.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = clf.predict(testDataCopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training done, lets see the performance of the training. Starting with the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got 53.4% accuracy. This is a very bad value. However, accuracy is not the best performance indicator when it comes to classification. Let's check the precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With precision being 53.58% and Recall being 50.8%, these are some bad values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the algorithm on a very basic configuration. Let's run the algorithm again with a different kernel and some other hyper-parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001, 0.0005, 0.005], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = svm.SVC(verbose=True)\n",
    "clf = GridSearchCV(svc, param_grid, verbose=True)\n",
    "\n",
    "%time clf.fit(trainDataCopy, trainLabels)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best results we got were for {'C': 1, 'kernel': 'linear'}. Which was also the default algorithm that ran first time. Let's see how we can improve it. We can try to extract some meaningful features. We can use PCA principal component analysis to extract 150 fundamental components of the image to feed into out support vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "clf = svm.SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, clf)\n",
    "\n",
    "#Train the model using the training sets\n",
    "model.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = model.predict(testDataCopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "\n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Accuracy: 61.6%, Precision: 60.8% and Recall 65.2%, we see a jump in the all performance measures by using a different kernel and using 150 principle component. Lets' try a combination of hyper-parameters again on this new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(trainDataCopy, trainLabels)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best configuration turns out to be with c = 5 and gamma being 0.005. Let's see the performance with this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "testPrediction = model.predict(testDataCopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "\n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Accuracy: 61.2%, Precision: 60.5, Recall: 64.4% Its nearly same. In fact, it is fractionally low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move onto the next algorithm i.e <b>K-nearest neighbor</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# taking a 50 as a random value for nearest neighbor. \n",
    "knn = KNeighborsClassifier(n_neighbors = 50)\n",
    "knn.fit(trainDataCopy, trainLabels)\n",
    "testPrediction = knn.predict(testDataCopy)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting results: Accuracy: 60%, Precision: 57%, and Recall: 80%. It is better then the first attempt of SVM specially value of Recall. Also, the value of k was randomly selected. Let's see where do we get the best accuracy, precision and recall for different values of k. We can see the accuracy for k value between 1 to 1000 with a interval of 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kValue = []\n",
    "accuracyScore = []\n",
    "precisionScore = []\n",
    "recallScore = []\n",
    "index=0\n",
    "acc=0\n",
    "\n",
    "# checking the performance of the algorithm on different values of k \n",
    "for k in tqdm(range(1, 1000, 10)):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(trainDataCopy, trainLabels)\n",
    "    testPrediction = knn.predict(testDataCopy)\n",
    "    accuracyScore.append(metrics.accuracy_score(testLabels, testPrediction))\n",
    "    precisionScore.append(metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "    recallScore.append(metrics.recall_score(testLabels, testPrediction, pos_label=32))\n",
    "    kValue.append(k)\n",
    "\n",
    "plt.figure(figsize=(15,16))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(kValue, accuracyScore)\n",
    "plt.title('Accuarcy score for different value of k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuarcy')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(kValue, precisionScore)\n",
    "plt.title('Precision score for different value of k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(kValue, recallScore)\n",
    "plt.title('Recall score for different value of k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are weird, The maximum values for performance indicators are Accuracy: 61.4% for k value 71, Precision: 59.8% for k value 61, and Recall: 94.4% for k value 850. Although, we have good score of Accuracy and Precision for k values ranging from 31 to 301. But Recall showed some weird behavior. Recall describes how good the model is at predicting the positive class when the actual outcome is positive. Let's get the z sore which is a harmonic mean of precision and recall and  Plot ROC curve to get a better insight into the overall performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the f1 score for k = 71\n",
    "knn = KNeighborsClassifier(n_neighbors = 71)\n",
    "knn.fit(trainDataCopy, trainLabels)\n",
    "testPrediction = knn.predict(testDataCopy)\n",
    "\n",
    "print(\"F1 Score:\", metrics.f1_score(testLabels, testPrediction, pos_label=32))\n",
    "\n",
    "print(\"The AUC for the curve is:\", metrics.roc_auc_score(testLabels, testPrediction))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(testLabels, testPrediction, pos_label=32)\n",
    "\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rerun the KNN with the PCA. Taking only 150 principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 150 principal components out of the image\n",
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "# Setting KNN with k=71\n",
    "knn = KNeighborsClassifier(n_neighbors = 71)\n",
    "# Creating a pipeline\n",
    "model = make_pipeline(pca, knn)\n",
    "\n",
    "#Train the model using the training sets\n",
    "model.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = model.predict(testDataCopy)\n",
    "\n",
    "print(\"F1 Score:\", metrics.f1_score(testLabels, testPrediction, pos_label=32))\n",
    "\n",
    "print(\"The AUC for the curve is:\", metrics.roc_auc_score(testLabels, testPrediction))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(testLabels, testPrediction, pos_label=32)\n",
    "\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of SVM we saw a jump with PCA but for KNN the AUC decreased. Just to confirm this holds for all the values of the PCA, lets run the code again for different values of number of principal components like 100, 200, 500, 750, 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking the value of KNN with differentvalue of PCA\n",
    "PCAValues = [100, 200, 500, 750, 1000]\n",
    "\n",
    "for pcaNumber in PCAValues:\n",
    "    # Creating 150 principal components out of the image\n",
    "    pca = RandomizedPCA(n_components=pcaNumber, whiten=True, random_state=42)\n",
    "\n",
    "    # Setting KNN with k=71\n",
    "    knn = KNeighborsClassifier(n_neighbors = 71)\n",
    "    # Creating a pipeline\n",
    "    model = make_pipeline(pca, knn)\n",
    "\n",
    "    #Train the model using the training sets\n",
    "    model.fit(trainDataCopy, trainLabels)\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    testPrediction = model.predict(testDataCopy)\n",
    "\n",
    "    print(\"For PCA number: \", pcaNumber)\n",
    "    print(\"F1 Score:\", metrics.f1_score(testLabels, testPrediction, pos_label=32))\n",
    "    print(\"The AUC for the curve is:\", metrics.roc_auc_score(testLabels, testPrediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't observe any change in the performance of the KNN with the change in the PCA numbers. So, we can move on to next algorithm i.e random forest. Random forest is ensemble algorithm, meaning that these algorithm improve the machine learning results by combining several models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, lets run the random forest without any hyper-parameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the randomforest library from the scikit learn\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "#Train the model using the training sets\n",
    "rf.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = rf.predict(testDataCopy)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Accuracy: 59%, Precision: 61.6%, Recall: 47.6% we did not get any performance improve with the random forest algorithm. Let's see if we get any performance increase after PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "rf = RandomForestClassifier()\n",
    "model = make_pipeline(pca, rf)\n",
    "\n",
    "#Train the model using the training sets\n",
    "model.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = model.predict(testDataCopy)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA the performance decreased. So, lets drop the idea of using the PCA for this algorithm and move onto hyperparameter tuning. Starting with the number of trees. Let's see how we get the performance change through AUC of ROC curve, if we increase the number of trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different number of trees in random forest \n",
    "n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200, 500, 1000]\n",
    "AUCScore = []\n",
    "for estimator in tqdm(n_estimators):\n",
    "    rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n",
    "    rf.fit(trainDataCopy, trainLabels)\n",
    "    testPrediction = rf.predict(testDataCopy)\n",
    "    metrics.roc_auc_score(testLabels, testPrediction)\n",
    "    AUCScore.append(metrics.roc_auc_score(testLabels, testPrediction))\n",
    "    \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(n_estimators, AUCScore)\n",
    "plt.title('AUC for different number of trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next hyperparameter is <i>max_depth</i>: represents the depth of each tree in the forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# depth of trees ranging from 1 to 64\n",
    "max_depths = np.linspace(1, 64, 64, endpoint=True)\n",
    "\n",
    "AUCScore = []\n",
    "for max_depth in tqdm(max_depths):\n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1)\n",
    "    rf.fit(trainDataCopy, trainLabels)\n",
    "    testPrediction = rf.predict(testDataCopy)\n",
    "    metrics.roc_auc_score(testLabels, testPrediction)\n",
    "    AUCScore.append(metrics.roc_auc_score(testLabels, testPrediction))\n",
    "    \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(max_depths, AUCScore)\n",
    "plt.title('AUC for different number of trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the problem vary a lot but we can see a decrease of value on average AUC right from start. Moving on to next hyperparameter: <i>min_samples_split</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# depth of trees ranging from 1 to 64\n",
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "AUCScore = []\n",
    "\n",
    "for min_samples_split in tqdm(min_samples_splits):\n",
    "    rf = RandomForestClassifier(min_samples_split=min_samples_split)\n",
    "    rf.fit(trainDataCopy, trainLabels)\n",
    "    testPrediction = rf.predict(testDataCopy)\n",
    "    metrics.roc_auc_score(testLabels, testPrediction)\n",
    "    AUCScore.append(metrics.roc_auc_score(testLabels, testPrediction))\n",
    "    \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(min_samples_splits, AUCScore)\n",
    "plt.title('AUC for different number of trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a sharp decrease in performance post 20%. Moving to the last hyperparameter: <i>min_samples_leaf</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# depth of trees ranging from 1 to 64\n",
    "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "AUCScore = []\n",
    "\n",
    "for min_samples_leaf in tqdm(min_samples_leafs):\n",
    "    rf = RandomForestClassifier(min_samples_leaf=min_samples_leaf)\n",
    "    rf.fit(trainDataCopy, trainLabels)\n",
    "    testPrediction = rf.predict(testDataCopy)\n",
    "    metrics.roc_auc_score(testLabels, testPrediction)\n",
    "    AUCScore.append(metrics.roc_auc_score(testLabels, testPrediction))\n",
    "    \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(min_samples_leafs, AUCScore)\n",
    "plt.title('AUC for different number of trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to <i>min_samples_split</i>, <i>min_samples_leaf</i> also see a sharp drop after 20%. Now let's compile best of all the hyperparameter value into one training. n_estimators = 500, max_depth = 10, min_samples_splits = 0.2, min_samples_leafs = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=0.2, min_samples_leaf = 0.1, n_jobs=-1)\n",
    "\n",
    "#Train the model using the training sets\n",
    "rf.fit(trainDataCopy, trainLabels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "testPrediction = rf.predict(testDataCopy)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(testLabels, testPrediction)) \n",
    "print(\"Precision:\", metrics.precision_score(testLabels, testPrediction, pos_label=32))\n",
    "print(\"Recall:\", metrics.recall_score(testLabels, testPrediction, pos_label=32))\n",
    "\n",
    "print(\"The AUC for the curve is:\", metrics.roc_auc_score(testLabels, testPrediction))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(testLabels, testPrediction, pos_label=32)\n",
    "\n",
    "plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post hyper parameter tuning we see a little jump in performance with Accuracy: 62.4%, Precision: 61.7% and Recall: 65.6%. The AUC we get with ROC curve has shown the best result yet with AUC being 63.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diving into deep neural network, Let's start with a simple arrangement of convolution neural network to identify the food images. We will increase the complexity of the problem gradually. Starting with building the model. But before that, we need to scale and reshape our data to different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `StandardScaler` from `sklearn.preprocessing`\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(trainDataCopy)\n",
    "\n",
    "# Scale the train set\n",
    "trainDataCopy = scaler.transform(trainDataCopy)\n",
    "\n",
    "# Scale the test set\n",
    "testDataCopy = scaler.transform(testDataCopy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataCopy = trainDataCopy.reshape(1500,100,100,1)\n",
    "testDataCopy = testDataCopy.reshape(500,100,100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, lets convert the data to the binary form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "for i in range(trainLabels.shape[0]):\n",
    "    if(trainLabels[i][0] == 32):\n",
    "        trainLabels[i][0] = 0\n",
    "    else:\n",
    "        trainLabels[i][0] = 1\n",
    "    \n",
    "for i in range(testLabels.shape[0]):\n",
    "    if(testLabels[i][0] == 32):\n",
    "        testLabels[i][0] = 0\n",
    "    else:\n",
    "        testLabels[i][0] = 1    \n",
    "        \n",
    "# trainLabels = to_categorical(trainLabels)\n",
    "# testLabels = to_categorical(testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we have been missing out on the previous steps is scaling the data. Scaling the data impact the performance a lot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(100,100,1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "model.fit(trainDataCopy, trainLabels, validation_data=(testDataCopy, testLabels), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate on the test data\n",
    "score = model.evaluate(testDataCopy, testLabels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67% accuracy is nice but we can go a little further by trying to do the classification on the color images and see the performance. Reading the data and saving the pixel values in HDF5 files for quick read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following approach to the problem is inspired by the keras blog: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html . Also, following this blog we are going to incorporate a lot of new things which we haven't used yet like image transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with data augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rotation_range is a value in degrees (0-180), a range within which to randomly rotate pictures\n",
    "* width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally\n",
    "* rescale is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor.\n",
    "* shear_range is for randomly applying shearing transformations\n",
    "* zoom_range is for randomly zooming inside pictures\n",
    "* horizontal_flip is for randomly flipping half of the images horizontally --relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n",
    "* fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img(r'../../data/raw/food-101/imagesCrop100x100Color/apple_pie/543253.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 100, 100)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 100, 100)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir=r'../../data/raw/food-101/preview', save_prefix='apple_pie', save_format='jpg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we get --this is what our data augmentation strategy looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "for imageName in glob.glob(r'../../data/raw/food-101/preview/*.jpg'): #assuming JPG\n",
    "    display(Image(filename=imageName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the transformed data, let's make a new image dataset storing new transformed Data. Also, instead of directly jumping into 100 classification, let's start with only 6 classes. This will help us to come to the conclusion quicker. Also, we are only interested in transforming the training data mentioned in Metadata.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting on only the first six classes of food items\n",
    "def extract(keys, dictionary):\n",
    "    return dict((k, dictionary[k]) for k in keys if k in dictionary)\n",
    "\n",
    "# Reading the train json file and converting the data to a dictionary\n",
    "with open(r'../../data/raw/food-101/meta/train.json', 'r') as f:\n",
    "    trainDataDictionary = json.load(f)\n",
    "    \n",
    "# creating s subset of dictionary containing only limited keys.\n",
    "trainDictionarySubSet = extract(['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad'], trainDataDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the destination\n",
    "destinationPath = r'..\\..\\data\\raw\\food-101\\smallersample\\imagesCrop100x100Color-transformed'\n",
    "\n",
    "for key in tqdm(trainDictionarySubSet.keys()):\n",
    "    for val in trainDictionarySubSet[key]:\n",
    "        imagePath = r'../../data/raw/food-101/smallersample/imagesCrop100x100Color/' + val + '.jpg'\n",
    "        imageName = val.split('/')[1]\n",
    "        foodType = val.split('/')[0]\n",
    "        val = val + '.jpg'\n",
    "        newDestinatioPath = os.path.join(destinationPath, foodType)\n",
    "        img = load_img(imagePath)\n",
    "        imageArray = img_to_array(img)  # this is a Numpy array with shape (3, 100, 100)\n",
    "        imageArray = imageArray.reshape((1,) + imageArray.shape)  # this is a Numpy array with shape (1, 3, 100, 100)\n",
    "        \n",
    "        # writing the transformed data to new folder\n",
    "        i = 0\n",
    "        for batch in datagen.flow(imageArray, batch_size=1, save_to_dir=newDestinatioPath, save_prefix=imageName, save_format='jpg'):\n",
    "            i += 1\n",
    "            if i > 20:\n",
    "                break  # otherwise the generator would loop indefinitely        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant! now we have new 6 food items with transformed data. However, instead of keeping the image in a directory, we can generate them dynamically during the training. But before that lets start with writing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(200, 200, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the binary_crossentropy loss to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data. As discussed earlier that we we don't need to create separate folder for data transformation. We can achieve it dynamically. To simplify the process we are going to use a function called <b>flow_from_directory</b>. Which loads images directly from the directory, we just need to separate the train and test data in different directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "    \n",
    "dst_dir = \"../../data/raw/food-101/smallersample/train/\"\n",
    "\n",
    "for key in tqdm(trainDictionarySubSet.keys()):\n",
    "    for val in trainDictionarySubSet[key]:\n",
    "        imagePath = r'../../data/raw/food-101/imagesCrop/' + val + '.jpg'\n",
    "        foodType = val.split('/')[0]\n",
    "        newDestinatioPath = os.path.join(dst_dir, foodType)\n",
    "        shutil.copy(imagePath, newDestinatioPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly adding data to test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the test json file and converting the data to a dictionary\n",
    "with open(r'../../data/raw/food-101/meta/test.json', 'r') as f:\n",
    "    testDataDictionary = json.load(f)\n",
    "    \n",
    "# creating s subset of dictionary containing only limited keys.\n",
    "testDictionarySubSet = extract(['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad'], testDataDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = \"../../data/raw/food-101/smallersample/test/\"\n",
    "\n",
    "for key in tqdm(testDictionarySubSet.keys()):\n",
    "    for val in testDictionarySubSet[key]:\n",
    "        imagePath = r'../../data/raw/food-101/imagesCrop/' + val + '.jpg'\n",
    "        foodType = val.split('/')[0]\n",
    "        newDestinatioPath = os.path.join(dst_dir, foodType)\n",
    "        shutil.copy(imagePath, newDestinatioPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! moving onto dynamically preparing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../../data/raw/food-101/smallersample/train/',  # this is the target directory\n",
    "        target_size=(200, 200),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '../../data/raw/food-101/smallersample/test/',\n",
    "        target_size=(200, 200),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "model.save_weights('../../model/firstColorImageTry.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 88.4% accuracy, thats a really good start. Let's rerun the same model but for all 101 food items. Starting with moving test and train data for all food items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading the train json file and converting the data to a dictionary\n",
    "with open(r'../../data/raw/food-101/meta/train.json', 'r') as f:\n",
    "    trainDataDictionary = json.load(f)\n",
    "    \n",
    "dst_dir = \"../../data/raw/food-101/smallersample/train/\"\n",
    "\n",
    "for key in tqdm(trainDataDictionary.keys()):\n",
    "    for val in trainDataDictionary[key]:\n",
    "        imagePath = r'../../data/raw/food-101/imagesCrop/' + val + '.jpg'\n",
    "        foodType = val.split('/')[0]\n",
    "        newDestinatioPath = os.path.join(dst_dir, foodType)\n",
    "        shutil.copy(imagePath, newDestinatioPath)\n",
    "        \n",
    "# Reading the test json file and converting the data to a dictionary\n",
    "with open(r'../../data/raw/food-101/meta/test.json', 'r') as f:\n",
    "    testDataDictionary = json.load(f)\n",
    "    \n",
    "dst_dir = \"../../data/raw/food-101/smallersample/test/\"\n",
    "\n",
    "for key in tqdm(testDataDictionary.keys()):\n",
    "    for val in testDataDictionary[key]:\n",
    "        imagePath = r'../../data/raw/food-101/imagesCrop/' + val + '.jpg'\n",
    "        foodType = val.split('/')[0]\n",
    "        newDestinatioPath = os.path.join(dst_dir, foodType)\n",
    "        shutil.copy(imagePath, newDestinatioPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data ready! lets rerun the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import json\n",
    "import os\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(200, 200, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides = 2, padding = 'valid'))\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides = 2, padding = 'valid'))\n",
    "\n",
    "# model.add(Dropout(0.2))\n",
    "# # model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides = 2, padding = 'valid'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VGG 16\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(64, (3, 3), padding='valid', activation=\"relu\", input_shape=(200, 200, 3)))\n",
    "# model.add(Conv2D(64, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(Conv2D(128, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))\n",
    "\n",
    "# model.add(Conv2D(256, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(Conv2D(256, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(Conv2D(256, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))\n",
    "\n",
    "# model.add(Conv2D(512, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(Conv2D(512, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(Conv2D(512, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))\n",
    "\n",
    "# # model.add(Conv2D(512, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# # model.add(Conv2D(512, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# # model.add(Conv2D(512, (3, 3), padding='valid', activation=\"relu\"))\n",
    "# # model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))\n",
    "    \n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../../data/raw/food-101/smallersample/train/',  # this is the target directory\n",
    "        target_size=(200, 200),  # all images will be resized to 200x200\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '../../data/raw/food-101/smallersample/test/',\n",
    "        target_size=(200, 200),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=3750 // batch_size,\n",
    "        epochs=30,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1250 // batch_size)\n",
    "\n",
    "model.save_weights('../../model/allDataTry.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a problem. Accuracy is 99.01% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the values accuracy with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to remember\n",
    "Neural networks.\n",
    "\n",
    "* images where we will lose the information if we crop to make it rectangular\n",
    "* using an algorithm to find the important part of food image which can be used to check the performance as well specially for cropped images.\n",
    "* Checking performance with or without data augmentation\n",
    "* after making it square, check performance with or without making all images of same size\n",
    "* use 5 different classifier atleast to know the performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
